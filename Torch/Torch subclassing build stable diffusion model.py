import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt


import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# å‡è¨­æˆ‘å€‘æœ‰ä¸€å€‹ DataFrame å«åš data_frame
# é€™è£¡ç”Ÿæˆä¸€å€‹éš¨æ©Ÿæ•¸æ“šæ¡†ä½œç‚ºç¤ºä¾‹
path = r"C:\Users\u048\0002 G1_A1.xlsx"
df = pd.read_excel(path)
data_frame = df.iloc[:,[1,3]]


# æ¨™æº–åŒ–æ•¸æ“š
scaler = StandardScaler()
data = scaler.fit_transform(data_frame.values)
data = torch.tensor(data, dtype=torch.float32)


# =============================================================================
# æ“´æ•£æ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šéåŠ å™ªè²å’Œå»å™ªéç¨‹ä¾†é€²è¡Œç”Ÿæˆã€‚è¨“ç·´éç¨‹ä¸­ï¼Œæ¨¡å‹å­¸æœƒäº†å¦‚ä½•å¾åŠ äº†å™ªè²çš„æ•¸æ“šä¸­é æ¸¬å‡ºå™ªè²ï¼Œ
# è€Œç”Ÿæˆéç¨‹å‰‡æ˜¯åå‘åŸ·è¡Œå»å™ªéç¨‹ï¼Œå¾éš¨æ©Ÿå™ªè²ä¸­ç”Ÿæˆæ–°çš„æ•¸æ“šã€‚é€™ç¨®æ–¹æ³•èˆ‡å‚³çµ±çš„ç”Ÿ
# =============================================================================


# å®šç¾©    ï¼ˆç°¡åŒ–ç‰ˆï¼‰
class SimpleUNet(nn.Module):
    def __init__(self, input_dim):
        super(SimpleUNet, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
        )
        self.middle = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
        )
        self.decoder = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, input_dim),
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.middle(x)
        x = self.decoder(x)
        return x

# å®šç¾©æ“´æ•£éç¨‹
def q_sample(data, t, noise_level):
    """æ ¹æ“šæ™‚é–“æ­¥é•· t å¢åŠ å™ªè²"""
    noise_scale = noise_level * t  # è®“å™ªè²éš¨æ™‚é–“è®ŠåŒ–
    return data * (1 - noise_scale) + noise_scale * torch.randn_like(data) #è¾“å…¥ä¸ä¼ å…¥å‚æ•°sizeç›¸åŒçš„æ»¡è¶³æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„éšæœºæ•°å­—tensor
    # torch.randn_like()æ˜¯ä¸€ä¸ªPyTorch å‡½æ•°ï¼Œå®ƒè¿”å›ä¸€ä¸ªä¸è¾“å…¥å¼ é‡å¤§å°ç›¸åŒçš„å¼ é‡ï¼Œå…¶ä¸­å¡«å……äº†å‡å€¼ä¸º0 æ–¹å·®ä¸º1 çš„æ­£æ€åˆ†å¸ƒçš„éšæœºå€¼
def noise_prediction_loss(model, x_noisy, t, clean_data, noise_level):
    """è¨ˆç®—æå¤±ï¼šæ¨¡å‹é æ¸¬å™ªè²"""
    pred_noise = model(x_noisy)
    true_noise = (x_noisy - clean_data) / noise_level
    return nn.MSELoss()(pred_noise, true_noise)

# è¨“ç·´èˆ‡ç”Ÿæˆå‡½æ•¸
def train_diffusion_model(data, num_steps=1000, noise_level=0.1, epochs=100, lr=1e-4):
    input_dim = data.shape[1]
    model = SimpleUNet(input_dim).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()

        # éš¨æ©Ÿé¸æ“‡æ™‚é–“æ­¥æ•¸ t
        t = torch.randint(1, num_steps, (data.shape[0],)).float().to(device) / num_steps
        noisy_data = q_sample(data, t, noise_level)

        # è¨ˆç®—æå¤±ä¸¦åå‘å‚³æ’­
        loss = noise_prediction_loss(model, noisy_data, t, data, noise_level)
        loss.backward()
        optimizer.step()

        if epoch % 10 == 0:
            print(f"Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}")

    return model

def generate_samples(model, num_samples, input_dim, num_steps=1000, noise_level=0.1):
    model.eval()
    with torch.no_grad():
        # åˆå§‹åŒ–éš¨æ©Ÿå™ªè²
        samples = torch.randn(num_samples, input_dim).to(device)

        # é€æ­¥å»å™ªéç¨‹
# =============================================================================
#         2. ç‚ºä½•ç”Ÿæˆæ™‚éœ€è¦å¾æœ€å¤§ğ‘‡æœŸé–‹å§‹é€æ­¥é‚„åŸï¼Ÿ
#         æ“´æ•£æ¨¡å‹çš„ç”Ÿæˆéç¨‹æ˜¯å¾å®Œå…¨çš„éš¨æ©Ÿå™ªè²é–‹å§‹ï¼Œé€æ­¥é‚„åŸæˆçœŸå¯¦æ•¸æ“šã€‚é€™å€‹éç¨‹éµå¾ª é€†å‘é¦¬å¯å¤«éˆï¼ˆReverse Markov Chainï¼‰ï¼Œå³ï¼š
#         é€™å€‹éç¨‹çš„æ ¸å¿ƒæ€æƒ³ï¼š
#         
#         å¾ç´”å™ªè² xt é–‹å§‹ï¼Œå› ç‚ºæ“´æ•£æ¨¡å‹è¨“ç·´æ™‚åŠ å™ªè²çš„æ–¹å¼é¡ä¼¼æ–¼æ¨¡æ“¬ã€Œæ•¸æ“šåˆ†ä½ˆ â†’ é«˜æ–¯åˆ†ä½ˆã€çš„éç¨‹ï¼Œæ‰€ä»¥è¦å¾å®Œå…¨éš¨æ©Ÿå™ªè²é–‹å§‹ç”Ÿæˆã€‚
#         
#         ä¸€æ­¥æ­¥å»å™ªé‚„åŸæ•¸æ“šï¼Œåœ¨æ¯å€‹æ™‚é–“æ­¥ ğ‘¡ï¼Œç”¨æ¨¡å‹é æ¸¬ç•¶å‰çš„å™ªè²ï¼Œä¸¦å¾ç•¶å‰çš„ ğ‘¥ğ‘¡ ä¸­æ¸›å»é€™å€‹å™ªè²ï¼Œé‚„åŸå‡º ğ‘¥ğ‘¡âˆ’1â€‹ ã€‚
#         æœ€çµ‚ ğ‘¥0 æœƒè®Šæˆæ¥è¿‘åŸå§‹æ•¸æ“šåˆ†ä½ˆçš„æ¨£æœ¬ã€‚
#         é€™é¡ä¼¼æ–¼ç”¨ã€Œé€æ­¥ä¿®æ­£ã€çš„æ–¹å¼ï¼Œå¾é›œè¨Šé‡å»ºæ¸…æ™°åœ–åƒï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§ç”Ÿæˆã€‚
# =============================================================================
        for t in reversed(range(num_steps)):
            t_tensor = torch.full((num_samples,), t / num_steps).float().to(device)
            pred_noise = model(samples)
            samples = samples - noise_level * pred_noise

        return samples.cpu().numpy()


# =============================================================================
# 3. ç‚ºä»€éº¼ä¸èƒ½ä¸€æ­¥åˆ°ä½ç”Ÿæˆï¼Ÿ
# æ“´æ•£æ¨¡å‹ä¸èƒ½ç›´æ¥ã€Œä¸€æ­¥åˆ°ä½ã€ç”Ÿæˆæ•¸æ“šï¼Œä¸»è¦æœ‰å¹¾å€‹åŸå› ï¼š
# å»å™ªéœ€è¦é€æ­¥ä¿®æ­£ï¼šå¦‚æœæˆ‘å€‘ç›´æ¥è®“æ¨¡å‹ç”Ÿæˆæœ€çµ‚æ•¸æ“š ğ‘¥0â€‹
#  ï¼Œè€Œä¸æ˜¯é€šéå»å™ªéç¨‹ï¼Œæ¨¡å‹å°±éœ€è¦å­¸ç¿’ä¸€å€‹è¤‡é›œçš„æ˜ å°„ï¼ˆå¾éš¨æ©Ÿå™ªè² â†’ çœŸå¯¦æ•¸æ“šï¼‰ï¼Œé€™æ¯”å­¸ç¿’ã€Œå±€éƒ¨å»å™ªã€å›°é›£å¾—å¤šã€‚
# æ¨¡å‹å­¸åˆ°çš„æ˜¯å™ªè²åˆ†ä½ˆï¼Œè€Œä¸æ˜¯ç›´æ¥çš„æ•¸æ“šåˆ†ä½ˆï¼šæ“´æ•£æ¨¡å‹è¨“ç·´çš„æœ¬è³ªæ˜¯å­¸ç¿’ã€Œä¸åŒæ™‚é–“æ­¥é©Ÿä¸‹çš„å™ªè²ã€ï¼Œè€Œä¸æ˜¯ç›´æ¥å­¸ç¿’æœ€çµ‚çš„æ•¸æ“šï¼Œå› æ­¤å®ƒéœ€è¦é€šéã€Œå»å™ªéˆã€ä¸€æ­¥æ­¥é‚„åŸæ•¸æ“šã€‚
# æ•¸æ“šæµå½¢ï¼ˆData Manifoldï¼‰éç·šæ€§è®ŠåŒ–ï¼šæ•¸æ“šçš„åˆ†ä½ˆé€šå¸¸æ˜¯é«˜åº¦éç·šæ€§çš„ï¼Œé€éé€æ­¥ä¿®æ­£æ¯”ç›´æ¥å­¸ç¿’ä¸€å€‹éç·šæ€§å‡½æ•¸ä¾†å¾—æ›´ç©©å®šã€‚
# =============================================================================


# è¨­ç½®åƒæ•¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_samples = 2000
input_dim = 2  # ä¸€ç¶­æ•¸æ“š


# æ¨™æº–åŒ–æ•¸æ“š
scaler = MinMaxScaler()
data = scaler.fit_transform(data_frame.values)
data = torch.tensor(data, dtype=torch.float32)


# # ç”Ÿæˆè¨“ç·´æ•¸æ“šï¼ˆä¾‹å¦‚ï¼šé«˜æ–¯åˆ†å¸ƒï¼‰
# true_data = np.random.normal(loc=0, scale=1, size=(num_samples, input_dim))
# train_data = torch.tensor(true_data, dtype=torch.float32).to(device)

# è¨“ç·´æ¨¡å‹
print("Training model...")
model = train_diffusion_model(data, epochs=10000)

# ç”Ÿæˆæ–°æ¨£æœ¬
print("Generating samples...")
generated_samples_trans = generate_samples(model, num_samples=500, input_dim = input_dim)
generated_samples = scaler.inverse_transform(generated_samples_trans) 

plt.scatter(generated_samples[:,0],generated_samples[:,1])

# # å¯è¦–åŒ–ç”Ÿæˆçµæœ
# plt.hist(generated_samples, bins=50, alpha=0.6, label="Generated Data")
# plt.hist(true_data, bins=50, alpha=0.6, label="True Data")
# plt.legend()
# plt.show()
