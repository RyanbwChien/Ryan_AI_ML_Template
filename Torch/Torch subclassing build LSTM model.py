import torch
import torch.nn as nn

# =============================================================================
# ç•¶ nn.LSTM çš„è¼¸å‡ºå‚³éåˆ°ä¸‹ä¸€å±¤ nn.LSTMï¼Œå®ƒæœƒåŒ…å«æ‰€æœ‰æ™‚é–“æ­¥çš„ hidden statesã€‚
# é€™ç­‰åŒæ–¼ Keras LSTM return_sequences=True çš„è¡Œç‚ºã€‚
# å¦‚æœåªæƒ³å–æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„ hidden stateï¼Œä½¿ç”¨ output[:, -1, :]ã€‚
# =============================================================================

# å®šç¾© Seq2Seq æ¨¡å‹
class Seq2SeqModel(nn.Module):
    def __init__(self, input_dim,input_data_time_step, hidden_dim, output_dim, num_timesteps):
        super(Seq2SeqModel, self).__init__()

        # LSTM å±¤ï¼šç¬¬ä¸€å±¤ LSTM
        self.lstm1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        
        self.flatten = torch.nn.Flatten()
        # RepeatVector ç­‰æ•ˆï¼šè¨­å®šæ™‚é–“æ­¥é•·
        self.num_timesteps = num_timesteps
        
        # LSTM å±¤ï¼šç¬¬äºŒå±¤ LSTM
        self.lstm2 = nn.LSTM(hidden_dim*input_data_time_step, 12, batch_first=True)

        # TimeDistributed ç­‰æ•ˆï¼šæ¯å€‹æ™‚é–“æ­¥æ‡‰ç”¨å…¨é€£æ¥å±¤
        self.fc = nn.Linear(12, output_dim)

    def forward(self, x):
        # è¼¸å…¥è³‡æ–™åˆ°ç¬¬ä¸€å±¤ LSTM
        x, (hn, cn) = self.lstm1(x)

        # ä½¿ç”¨ `unsqueeze()` å’Œ `expand()` æˆ– `view()`ï¼Œé€™æ¨£å¯ä»¥ä¿è­‰æ™‚é–“æ­¥é•·ç‚º num_timesteps
        batch_size, seq_len, hidden_dim = x.size()
        x = self.flatten(x)
# =============================================================================
#         # å‡è¨­åŸæœ¬çš„æ™‚é–“æ­¥æ•¸æ˜¯ 5ï¼Œè¦è®Šæˆ 3
#         x = x[:, -1:, :]  # å–æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„è¼¸å‡ºï¼Œå½¢ç‹€ (batch_size, 1, hidden_dim)
# =============================================================================

# =============================================================================
#         # è¤‡è£½åˆ°æŒ‡å®šæ™‚é–“æ­¥é•· (num_timesteps)
#         x = x.expand(-1, self.num_timesteps, -1)  # å½¢ç‹€è®Šç‚º (batch_size, num_timesteps, hidden_dim)
# 
# =============================================================================
        # è¤‡è£½åˆ°æŒ‡å®šæ™‚é–“æ­¥é•· (num_timesteps)
        x = x.unsqueeze(1).repeat(1, num_timesteps, 1)  # å½¢ç‹€è®Šç‚º (batch_size, num_timesteps, hidden_dim)
        # é€™è¡¨ç¤º (batch_size, features) â†’ (batch_size, 1, features)ï¼Œç›¸ç•¶æ–¼åœ¨ dim=1 å¢åŠ äº†ä¸€å€‹æ–°çš„ time_step=1 ç¶­åº¦ã€‚


# =============================================================================
#         ğŸ”¹ expand() vs repeat()
#         æ–¹æ³•	ä½œç”¨	è¨˜æ†¶é«”	é©ç”¨å ´æ™¯
#         expand(-1, new_timesteps, -1)	æ“´å±•å¼µé‡è¦–åœ–ï¼Œä¸è¤‡è£½æ•¸æ“š	âœ… ç¯€çœè¨˜æ†¶é«”	åƒ…è®€å–è³‡æ–™ï¼Œä¸ä¿®æ”¹å…§å®¹
#         repeat(1, new_timesteps, 1)	ç›´æ¥é‡è¤‡æ•¸æ“š	ğŸš« æ¶ˆè€—æ›´å¤šè¨˜æ†¶é«”	éœ€è¦ä¿®æ”¹æ•¸æ“šæ™‚
#         âœ… çµè«–
#         å¦‚æœä½ åªæ˜¯æƒ³ æ“´å±•æ™‚é–“ç¶­åº¦ï¼Œä½† ä¸ä¿®æ”¹æ•¸æ“šå…§å®¹ï¼Œä½¿ç”¨ expand() æ›´çœè¨˜æ†¶é«”ã€‚
#         å¦‚æœä½ éœ€è¦å°æ•¸æ“šåšè®Šæ›ï¼ˆä¾‹å¦‚ LSTM/Transformer è¨“ç·´æ™‚ï¼‰ï¼Œä½¿ç”¨ repeat() ç¢ºä¿ä¸å…±äº«è¨˜æ†¶é«”ã€‚
# =============================================================================


        # è¼¸å…¥åˆ°ç¬¬äºŒå±¤ LSTM
        x, (hn, cn) = self.lstm2(x)

        # æ‡‰ç”¨ TimeDistributed æ“ä½œï¼Œå°æ¯å€‹æ™‚é–“æ­¥ä½¿ç”¨å…¨é€£æ¥å±¤
        x = self.fc(x)

        return x

# å‡è¨­çš„è¼¸å…¥æ•¸æ“š (batch_size, timesteps, features)
input_data = torch.randn(192, 5, 15)  # 192 æ˜¯ batch_sizeï¼Œ5 æ˜¯ timestepsï¼Œ15 æ˜¯ features

# åˆå§‹åŒ–æ¨¡å‹
input_dim = 15
hidden_dim = 32
output_dim = 6
num_timesteps = 3
input_data_time_step= 5

model = Seq2SeqModel(input_dim, input_data_time_step, hidden_dim, output_dim, num_timesteps)

# æ¨¡å‹å‰å‘å‚³æ’­
output = model(input_data)

# é¡¯ç¤ºè¼¸å‡ºå½¢ç‹€
print("Output shape:", output.shape)
